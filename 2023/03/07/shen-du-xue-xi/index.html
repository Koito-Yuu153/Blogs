<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="深度学习笔记, Koito Yuu&#39;s Blogs">
    <meta name="description" content="    在机器学习中，一切的数据都可以看作是矩阵

Tensor：可以进行GPU加速计算的矩阵（任意维度），Tensorflow操作的基本呢单元
Tensor可以和numpy 进行相互转换
#直接转换
x.numpy()
#指定转换格式
x">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>深度学习笔记 | Koito Yuu&#39;s Blogs</title>
    <link rel="icon" type="image/png" href="/Blogs/favicon.png">
    
    <style>
        body{
            background-image: url(https://cdn.jsdelivr.net/gh/Tokisaki-Galaxy/res/site/medias/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Blogs/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Blogs/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Blogs/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Blogs/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Blogs/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Blogs/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Blogs/css/my.css">
<link rel="stylesheet" type="text/css" href="/Blogs/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Blogs/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Blogs/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/Blogs/css/reward.css">
    



    <script src="/Blogs/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.1.1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/Blogs/css/prism.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Blogs/" class="waves-effect waves-light">
                    
                    <img src="/Blogs/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Koito Yuu&#39;s Blogs</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Blogs/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Blogs/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Koito Yuu&#39;s Blogs</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Blogs/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Koito-Yuu153/Koito-Yuu-s-Blog.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Koito-Yuu153/Koito-Yuu-s-Blog.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/Blogs/medias/featureimages/21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">深度学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Blogs/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-03-07
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <pre class=" language-lang-yaml"><code class="language-lang-yaml">    在机器学习中，一切的数据都可以看作是矩阵
</code></pre>
<p>Tensor：可以进行GPU加速计算的矩阵（任意维度），Tensorflow操作的基本呢单元</p>
<p>Tensor可以和numpy 进行相互转换</p>
<pre class=" language-lang-python"><code class="language-lang-python">#直接转换
x.numpy()
#指定转换格式
x=tf.case(x,tf.float) #转换成浮点型矩阵
</code></pre>
<p>机器学习流程：（深度学习属于机器学习的一部分）</p>
<ul>
<li>数据获取</li>
<li>特征工程</li>
<li>建立模型 </li>
<li>评估与应用</li>
</ul>
<p>特征工程的作用</p>
<ul>
<li>数据特征决定了模型的上限</li>
<li>预处理和特征提取是最核心的</li>
<li>算法与参数选择决定了如何逼近这个上限</li>
</ul>
<p>深度学习相对于传统的机器学习主要解决了特征工程这一部分，搭建了非常好的神经网络模型，去找出最合适的特征</p>
<p>深度学习应用：</p>
<p>主要用于计算机视觉处理、人脸识别</p>
<p>计算量很大，因此在移动端支持不是很好，神经网络的参数为百万千万级别。</p>
<p>颜色通道：RGB是其中一种，有3个颜色通道，读出来是一个三维数组</p>
<p>一张图片的颜色是由RGB三个通道构成, 可以把一张图片上的每一个像素点看成一个对象, 这个对象又由RGB三种颜色叠加, 即用一个一维数组表示,假如我们有一张 m * n 个像素点的图片, 那么每一行有 n 个像素, 即每一行有 n 个一维数组, 即这一行是一个二维数组, 那一张图片又有 m 行, 那么我们就得到了 m 个二维数组, 这m 个二维数组构成了一个三维数组，如果将RGB看作一个整体的值那么图像也可以理解成二维数组。</p>
<p>python中的图像（三位数组）</p>
<p><img src="图像与3维数组.png" alt=""></p>
<p>红色代表 R <a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=通道&amp;spm=1001.2101.3001.7020">通道</a>亮度值, 绿色代表 G 通道亮度值, 蓝色代表 B 通道亮度值</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29304033/article/details/116722162">https://blog.csdn.net/qq_29304033/article/details/116722162</a></p>
<p>计算机视觉面临的挑战：</p>
<ul>
<li>目标的一部分被其他物体遮蔽</li>
<li>目标与背景的特征高度相似（背景混入）</li>
</ul>
<p>需要给计算机遮蔽或混入的数据，并告诉它答案（标签），训练计算机去识别这种情况</p>
<p>机器学习的常规套路：</p>
<ul>
<li>收集数据集并给定标签</li>
<li>训练一个分类器</li>
<li>测试，评估</li>
</ul>
<p>视觉任务中要识别的往往只是图像中的一小部分主体，剩余的部分会影响处理，因此要尽可能地排除掉</p>
<p>线性函数：</p>
<p>从输入到输出的映射</p>
<p>输入一个图像和参数，输出每个类别的得分</p>
<p>损失函数：</p>
<p>衡量分类的结果</p>
<p>损失函数的值相同并不代表两个模型相同</p>
<p>损失函数=数据损失+正则化惩罚项</p>
<p>我们总是希望模型不要太复杂，过拟合的模型是没用的</p>
<p>神经网络整体架构</p>
<p>层次结构、神经元、全连接、非线性</p>
<p>神经元个数对结果的影响</p>
<p>个数越多，效果越好，过拟合风险越大</p>
<p>正则化和激活函数</p>
<h1 id="Neural-network-神经（元）网络"><a href="#Neural-network-神经（元）网络" class="headerlink" title="Neural network 神经（元）网络"></a>Neural network 神经（元）网络</h1><h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><p><img src="数字图像.png" alt=""></p>
<p>What are the neurons ?  什么是神经元？</p>
<p>How are they connected ?  它们是如何连接在一起的？</p>
<p>以手写数字识别为例</p>
<p>在这里，我们先暂时把神经元理解成一个用来装数字的容器，装着一个0到1之间的数字，仅此而已。像上图，这个神经网络一开始的地方有很多神经元，分别对应了28x28输入图像的每一个像素，总计784个神经元，神经元中装着的数字代表对应像素的灰度值，0代表纯黑像素，1代表纯白像素。我们把神经元里装着的数叫做“激活值”（activation）。可以想象这么一个画面：激活值越大，那么这个神经元就点着越亮那么这784个像素就组成了神经网络的第一层。</p>
<p><img src="神经网络结构.png" alt=""></p>
<p>现在我们跳到网络的最后一层，这一层的十个神经元分别代表0~9这十个数字，它们的激活值，同理都处在0到1之间。这些事表示系统认为输入的图像对应着哪个数字的可能性。网络中间还有几层”隐藏层“。神经网络运作时，上层的激活值将决定下层的激活值。所以说神经网络处理信息的核心机制正是一层的激活值是通过怎样的运算，算出下一层的激活值的。某种程度上讲，它想模仿的是生物中神经元组成的网络，某些神经元的激发就会促使另一些神经元激发。</p>
<p>在介绍网络每层间如何影响，训练过程的数学原理之前，我们先讨论影响，凭什么我们就觉得这种层状结构可以做到只能判断。我们在期待什么？我们到底期望值这些中间层最好能做些什么呢？当我们人类在识别数字时，我们是在组合数字的各个部件，如下图：</p>
<p><img src="数字的组成.png" alt=""></p>
<p>在理想的情况下，我们希望倒数第二层中的各个神经元能分别对应一个笔画部件。这样一来，当我们输入9或者8这种带圈的数字时，某个神经元的激活值就会接近1。而且我并不特指某种样子的圈，我是希望所有位于图像顶部的圆圈团都能点亮这个神经元，这样一来从第三层到最后一层，我们只要学习哪些部件能组合出哪个数字即可。</p>
<p><img src="数字的组成2.png" alt=""></p>
<p>当然，这样一来我们就引来更多的问题，例如要如何识别这些部件，而且我还没提到上一层网络是如何影响下一层的，不过暂时让我们先把这个话题讨论完。</p>
<p>识别圆圈的任务同理可以拆分成更细微的问题，一种合理的方法是首先识别出数字图形中更小的边，比如像1、4、7中的这种长条，就是一条长边嘛，或者把它当做几条短边组合起来的图案也可以。于是我们希望，也许神经网络第二层的各个神经元，就能对应上这些各种各样的短边。</p>
<p><img src="数字的组成3.png" alt=""></p>
<p>没准当9的数字图像输入进来的时候，它就能把所有关联短边的神经元都点亮，接着就能点亮对应顶部圆圈和长竖条的神经元，最后就能点亮输出层对应9的神经元，至于不咱们的网络是否能做到这一步，等解释完网络如何训练，再回头讨论吧。但这就是我们所希望的，希望这种层状结构能完成的目标，更进一步讲，加入神经网络真能识别出这类边缘和图案，它就能很好的运用到其他图像的识别任务上。甚至不光是图像识别，世界上各种人工智能的任务都可以转化为抽象元素，一层层的抽丝剥茧，比如说语音识别，也就是要从原音频中识别出特殊的声音组合成特定的音节，组合成单词，再组合成短语以及更加抽象的概念。</p>
<p>回到神经网络工作原理的话题上来，试想一下，你要设计上一层中的激活值到底如何让决定下一层中的激活值，我们需要设计一个机制，可以把像素拼成短边，把短边拼成图案或者把图案拼成数字等等。在数字识别这个例子中我们来放大关注其中一个，我们来设计，让第二层中的这一个神经元能够识别出图像中的这块区域是否存在一条边。</p>
<p><img src="数字识别4.png" alt=""></p>
<p>我们需要给这个神经元和第一层所有神经元间的每一条接线都赋上一个权重值，这些权重值都不过是数字而已。然后，我们拿起第一层所有的激活值，和它们对应的权重一起，算出它们的加权和。</p>
<p>如果我们把关注区域的权重赋为正值，而其他所有的权重一律赋为0，这样一来，对所有的像素值取加权和就只会累加我们关注区域的像素值了，如果你真的像识别出这里是否存在一条边界，只需要给周围的一圈像素赋予负的权重，这样当中间的像素亮周围的像素暗时加权和就能达到最大。（在周围一圈加负数权重的意义在于确定像素点亮的区域是仅限于你关注的区域还是周围的一大片都是亮的，当只有负权重围住的的区域亮的时候加权和最大，若负权重对应的像素点也是亮的，则加权和减小）。这样计算出来的加权和可以是任意大小，但在这个网络中，我们需要激活值都处在0到1之间，那么我们就可以顺其自然把这个加权和输进某个函数，将加权和挤压进0到1的区间内（归一化），其中有一个叫sigmoid的函数非常有用，它能将很大范围的实数压进（0，1）区间</p>
<p><img src="sigmoid函数.png" alt=""></p>
<p>所以这个神经元中的激活值，实际上就是对加权和到底有多正的打分。但有时，即使加权和大于0时，你也不想把神经元点亮，可能只有当和大于10时才让它激发，此时你就需要加上一个偏置值，保证不能随便激发，而我们只需要在加权和之后加上一个负10之类的数，再把它送进sigmoid函数中压缩即可。</p>
<p><img src="sigmoid函数2.png" alt=""></p>
<p>总而言之，权重告诉你这个第二层的神经元关注什么样的像素图案，偏置则告诉你加权和得有多大才能让神经元的激发变得有意义。</p>
<p>我们这就解说完了其中一个神经元，但是第二层中的每个神经元都会和第一层中的784个神经元相连接，每一个的784个接线上都带着一个权重，而且每个神经元都会在计算自己的加权和后加上自己的偏置再通过sigmoid压缩输出自己的结果，一下子要考虑的就多起来了，而且这单单时第一层和第二层之间的连接，别的层之间的连接还有它们分别自带的权重和偏置，一套下来整个网络的参数和计算量都是非常大的。整个网络有非常多个参数等着你去调整，从而带来不一样的结果。所以当我们讨论机器如何学习的时候，我们其实在讲，电脑应该如何设置这一大坨数字参数，才能让它正确地解决问题。</p>
<p>如果每个激活值都用sigmoid函数列出来会非常麻烦，通常采用另一种方法表示：我们把某一层中所有的激活值统一成一列向量，再把它和下一层间所有的权重放到一个矩阵中，矩阵的第n行就是这一层的所有神经元和下一层第n个神经元间所有连线的权重。这样权重矩阵和向量乘积的第n项就是这一层所有的激活值和下一层第n个神经元间连线权重的加权和，最后加上偏置值组成的向量，最后用sigmoid包起来（就是指对表达式结果向量中的每一项都取一次sigmoid）</p>
<p><img src="加权和简洁表示.png" alt=""></p>
<p><img src="加权和简洁表示2.png" alt=""></p>
<p>现在只要我们写一下权重矩阵和相应向量的符号，神经网络各层之间激活值的转化就可以清晰简洁明了了。</p>
<p>前面提过暂时把神经元看作数字的容器，实际上神经元中装着的值是取决于你的输入图像的，所以我们把神经元看作一个函数才更加准确，它输入的是上一层所有神经元的输出，而它的输出是一个0到1之间的值。其实神经网络就是一个函数，一个输入784个值，输出10个值的函数，不过这个函数极其复杂，用了大量权重参数偏置参数来识别特殊图案，又要循环不停地用矩阵乘法和sigmoid映射运算，但它终究只是个函数而已。而它的复杂程度可以稍微让人安点心，如果它没这么复杂的话，我们恐怕就不大能指望它的数字识别能多准了。</p>
<p>值得一提的是，sigmoid函数多应用于早期的网络，把加权和映射到0~1的区间内，来模仿生物学上的神经元是否激发。但现在的网络基本都不用sigmoid了，相比之下ReLU（Rectified Linear Unit  线性整流函数）更好训练，其作用是返回0和a的最大值，其中a就是函数的输入。当超过一个阈值的时候，ReLU就和恒定函数一样，而没过这个阈值，那就不激发，输出0。sigmoid并没有让训练结果变得更好，或者某种程度上讲它很难训练，后来有人拿ReLU试了试，结果发现在特别深的神经网络上效果特别的好。</p>
<p><img src="ReLU.png" alt=""></p>
<h2 id="梯度下降法介绍"><a href="#梯度下降法介绍" class="headerlink" title="梯度下降法介绍"></a>梯度下降法介绍</h2><p>梯度下降法是神经网络学习的基础，机器学习中很多其他技术也是基于这个方法。</p>
<p>现在我们想要这么一种算法，你可以给这个网络看一大堆训练数据，其中包括一堆不同的手写数字图像以及它们代表哪个数字的标记。算法会调整神经网络的各个权重和偏置值，以提高网络对训练数据的表现。我们希望这种分层结构可以让它举一反三，识别训练数据之外的图像。训练好网络后，我们会给它更多以前从未见过的带标记的数据作为测试，你就能看到它对这些新图像进行分类的准确度。</p>
<p>虽然机器会”学习“的说法很大胆，但当你实际看到它的工作原理之后，这听起来就不再像是疯狂的科幻场面，而更像是一道微积分习题了———-我是指这实际上是再找某个函数的最小值。</p>
<p>从概念上讲，我们认为每个神经元都与上一层的所有神经元相连接。决定其激活值的加权和中的权重，有点像是那些连接的强弱，而偏置值则表明神经元是否更容易被激活</p>
<p><img src="加权和.png" alt=""></p>
<p>在一开始，我们会完全随机地初始化所有的权重和偏置值，可想而知，这个网络对于给定的训练示例会表现得非常糟糕，毕竟它只是在做随机的判断。这时你就要定义一个”代价函数“来告诉电脑它输出了提高糟糕的结果，用更加数学的语言来说，你要讲每个垃圾输出激活值与你想要的值之间的差的平方加起来，我们称之为训练单个样本的”代价“（注：单个样本上代价也叫Loss”损失/误差“ by吴恩达）。注意一下，网络能对图像进行正确的分类时，这个平方和就比较小，但如果网络稀里糊涂找不着北的话，这个平方和就很大。</p>
<p>接下来你就要考虑手头上几万个训练样本中代价的平均值，而我们就用这个平均代价来评价这个网络有多糟糕，电脑应该有多内疚。但这东西挺复杂的，要记得网络本身不过是个函数，有784个输入值即像素的灰度，最后的输出值是10个数字，而所有的权重值和偏置值可以说就组成了这个函数的参数。而代价函数还要再抽象一层，所有的权重值和偏置值作为它的参数，它输出的是单个数值，来表示这些权重和偏置有多差劲。而且代价函数取决于网络对于上万个训练数据的综合表现。</p>
<p>但如果你只告诉电脑它有多糟糕，那并不是很有用。你还得告诉它怎么改变这些权重和偏置值才能进步。为了简化问题，我们先不去想一个有庞大变量的函数，而先考虑简单的一元函数，只有一个输入变量，只输出一个数字要怎么找输入值x，使得函数值最小呢？</p>
<p><img src="一元函数的最小值.png" alt=""></p>
<p>学过微积分的都知道，有时候你可以直接算出这个最小值，不过函数很复杂的话就不一定能写出来，对于神经网络这个超复杂的函数就更加不可能做到了。一个更灵活的技巧是先随便挑选一个输入值，然后考虑向左走还是向右走，函数值才会变小。准确地说，如果你找到了函数在这里的斜率，那么斜率为正就向左走，斜率为负就向右走。在每个点都这样子重复计算新的斜率，在适当地走一小步的话，你就会逼近函数的某个局部最小值。要注意，就算是一个很简单的一元函数，由于不知道一开始的输入值在哪个位置，最后你可能落到许多不同的坑里，而且无法保证你落到的局部最小值就是代价函数可能达到的全局最小值，我们的神经网络也会遇到这个问题。值得一提的是如果每步的大小和斜率成比例，那么在最小值附近斜率会越来越平缓，每步会越来越小，这样可以防止调过头。</p>
<p>想象一个更复杂的，两个输入一个输出的二元函数，输入空间可以想象成XY平面，代价函数是平面上方的曲面，现在我们不问函数的斜率，而是应该问，在输入控件内沿哪个方向走，才好让输出结果下降得最快。</p>
<p><img src="二元函数.png" alt=""></p>
<p>熟悉多元微分的人已经知道，函数的梯度指出了函数的最陡增长方向。即是说，按梯度的方向走，函数值增长得就最快，那么沿梯度的负方向走，函数值自然就降低得最快了。而且，这个梯度向量的长度就代表了这个最陡的斜坡到底有多陡。</p>
<p>也就是说，让函数值最小的算法只不过是先计算梯度，再按梯度反方向走一步，然后循环。想想把所有权重偏置都放到一个列向量里，那么代价函数的负梯度也不过是个向量。负梯度正是指出了在这个大到爆炸的函数输入空间内，具体如何改变每一项参数，才可以让代价函数的值下降得最快。</p>
<p><img src="梯度向量.png" alt=""></p>
<p>那么，对于这个我们特别设计的代价函数，更新权重和偏置来降低代价函数的值，意味着输入训练集的每一份样本的输出都更接近期待的真实结果，而不是一串10个的随机数组。要注意的是这个函数取了整个训练集的平均值，所有最小化的意思是：对所有样本得到的总体结果都会更好一点。</p>
<p>这个梯度算法是神经网络的核心，我们叫做反向传播算法【BP】。</p>
<p>当我们提到让网络学习，实质上就是让代价函数的值最小，而未来达到这个效果，代价函数非常有必要是平滑的，这样我们才能，每次挪动一点点，最后找到一个局部最小值。这也顺便解释了为什么人工神经元的激活值是连续的而非直接沿袭生物学神经元那种二元式的，要么激活要么非激活的取值模式。这种按照负梯度的倍数，不停调整函数输入值的过程就叫做梯度下降法。这是一种可以让你收敛到代价函数某一个局部最小值里的地方。</p>
<p>目前为止我们都值展示了二元函数的图像，毕竟人脑想象一个太高维度空间中的变动是在是强人所难，但其实我们还有一种漂亮的思路，不用借助空间。</p>
<p><img src="梯度向量2.png" alt=""></p>
<p>负梯度中的每一项都告诉了我们两件事：正负号明显在告诉我们输入向量的这一项该调大还是调小，但重要的是每一项的相对大小更告诉了我们改变哪个值的影响更大。所以这时你再去看这个梯度向量，就可以把它理解为各个权重偏置的相对重要度，标记出了改变哪个参数的性价比最高。</p>
<p>这也是理解方向的另一种方式，举个简单的例子，假设有个输入两个变量的二元函数，你算出这个函数再某一点的梯度是[3，1]，一种解读是，你站在这个点顺着这个梯度的方向移动，函数的数值增加得最快,变化率最大（为该梯度的模）</p>
<p><img src="梯度向量3.png" alt=""></p>
<p>当你讲这个函数的曲面画出来时，可以看到沿着这个向量的方向走函数值增长最快。</p>
<p><img src="梯度向量4.png" alt=""></p>
<p>但我们还有另外一种解读，即<strong>第一个变量的重要性是第二个变量的3倍</strong>。也就是说，至少在这块取值区域内，改变x的值会造成更大（3倍）的影响。</p>
<p>好吧，我们回到对网络的讨论中来，小结一下。神经网络本身是一个多个输入和多个输出的函数，由各种加权和所定义。代价函数则是更复杂一层，讲所有权重偏置作为输入，通过训练数据，得出一个对网络糟糕程度的评分。而代价函数的梯度，则比上边还要复杂一层，告诉我们如何让微调群众偏置的值才可以让代价函数的结果改变得最快。也就是可以理解为，改变了哪些权重造成的影响最大。</p>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p><img src="反向传播算法.png" alt=""></p>
<p>开始讲解时就让我们我们先忘记上面的公式，完全抛弃所有符号，一步步解释每一个训练样本会对权重偏置的调整造成怎样的影响。</p>
<p>因为代价函数牵扯到对成千上万个训练样本的代价取平均值，所以我们调整每一步梯度下降用的权重偏置也会基于所有的训练样本——原理上时这么说。但未来计算效率，之后咱们会讨个巧，从而不必每部都非得要计算所有的训练样本。还需要说明一点，我们现在只关注一个训练样本，比如一张2的手写数字图片，这一个训练样本会对调整权重和偏置造成怎样的影响呢？</p>
<p><img src="反向传播算法2.png" alt=""></p>
<p>现在假设网络还没有完全训练好，那么输出层的激活值看起来就很随机，我们并不能直接改动这些激活值，只能改变权重和偏置值，但记住我们想要输出层出现怎样的变动还是很有用的。因为我们希望图像最终的分类结果是2，我们希望输出层中对应2 的神经元的激活值变大，其他神经元的数值变小，并且变动的大小应该与现在值和目标值之间的差呈正比。例如下图中增加数字2神经元的激活值，就应该比减少数字8神经元的激活值来得重要，因为后者已经很接近它的目标（0）了。</p>
<p><img src="反向传播算法3.png" alt=""></p>
<p>那好，我们更近一步，关注一下2对应的神经元，我们要让这里面的激活值变大。还记得吗？这个激活值是把前一层所有激活值的加权和加上一个偏置，再通过sigmoid、ReLU之类的挤压函数，最后算出来的吧。所有要增加这个激活值，二目有三条大路可走，一增加偏置，而增加权重，三改变前一层的激活值。</p>
<p>先来看看如何调整权重，各个权重它们的影响力各不相同，连接前一层最亮神经元的权重影响力也最大，因为这些权重会与大的激活值相乘。所有至少对于这样一个训练样本而言，增大了这几个权重值对最终代价函数造成的影响就比增大连接暗淡神经元的权重所造成的影响要大上很多倍。请记住当我们说到梯度下降的时候，我们并不只看每个参数是该增大还是减小，我们还该哪个参数的性价比最高。</p>
<p>对于改变前一层的激活值这一方法，更具体地说，如果所有正权重连接的神经元更亮，所有负权重连接的神经元更暗的话，那么数字2的神经元就会更强烈地激发，和改权重的时候类似，我们想造成更大的影响，就要依据权重的大小对激活值做出成比例的改变，当然我们并不能直接改变激活值，我们手头只能控制权重和偏置。但就光对输出层来说，记住我们期待的变化还是很有帮助的。</p>
<p>不过别忘了，从全局上看，这只不过是数字2的神经元所期待的变化。我们还需要输出层其余的神经元的激发变弱，但这其余的每个输出神经元对于如何让改变倒数第二层都由各自的想法。所以我们会把数字2神经元的期待和别的输出神经元的期待全部加起来，作为对如何改变倒数第二层神经元的指示。这些期待不仅是对应的权重的倍数，也是每个神经元激活值改变量的倍数。这其实就是再实现”反向传播“的理念了。我们把所有期待的改变加起来，就得到了一喜欢对倒数第二层改动的变化量，有了这些，我们就可以重复这个过程，改变倒数第二层神经元激活值的相关参数，从后一层到前一层，一直把这个过程循环到第一层。</p>
<p>放眼大局，还记得我们只是再讨论单个训练样本对所有权偏置的影响吗？如果我们只关注那个2的要求，最后网络只会把所有图像都分类成2，所以你要对其他所有的训练样本同样地过一遍反向传播，记录下每个样本想怎样修改权重与偏置，最后取一个平均值。</p>
<p><img src="反向传播算法4.png" alt=""></p>
<p>顺带一提，实际操作中，如果梯度下降的每一步都用每一个训练样本来计算的话，拿华东时间就太长了，所以我们一般会这样做：首先把训练样本打乱，然后分成很多组minibatch，每个minibatch就暂且当它包含100个样本好了，然后你算出这个minibatch下降的一步，这不是代价函数真正的梯度，毕竟计算真实梯度得用上所有的样本而非这个子集，所以这也不是找出代价函数局部最小值最高效的一步，然而，每个minibatch都会给你一个不错的近似，而且更重要的是，你的计算量也会减轻不少，这个技巧叫做随机梯度下降。</p>
<p>小结一下，反向传播算法算的是单个训练样本想怎样修改权重与偏置，不仅是说每个参数应该变大还是变小，还包括了这些变化的比例是多大，才能最快地降低代价，真正的梯度下降得对好几万个训练范例都这操作，然后对这些变化值取平均，但算起来太慢了，所以你会先把所有的样本分到各个minibatch中去，计算一个minibatch来作为梯度下降的一步，计算每个minibatch的梯度，调整参数，不断循环，最终你就会收敛到代价函数的一个局部最小值上。此时就可以说你的神经网络对付训练数据已经很不错了。</p>
<p>在实际设定神经网络的权重和偏置的初始值时，正态分布是一个有用的工具，使用服从这个分布的随机数，容易取得好的结果。</p>
<p>柯西-施瓦茨不等式：</p>
<script type="math/tex; mode=display">
- \left | a \right | \left | b \right | \le a\cdot b\le \left | a \right | \left | b \right |</script><p>向量内积（点积、数量积）：</p>
<script type="math/tex; mode=display">
a\cdot b=\left | a \right | \left | b \right | \cos \theta = a_{1}b_{1}+  a_{2}b_{2}\left ( \theta 为a、b的夹角 \right )</script><p>向量积（叉积、外积）：</p>
<script type="math/tex; mode=display">
a\times b=det\begin{bmatrix}
 i & j &k \\
 a_{x}   & a_{y}  & a_{z} \\
 b_{x}   & b_{y}  & b_{z} 
\end{bmatrix}</script><p>根据柯西-施瓦茨不等式可以得出以下结论</p>
<ul>
<li>当两个向量方向相反时，内积取得最小值</li>
<li>当两个向量不平行时，内积取得平行时的中间值</li>
<li>当两个向量方向相同时，内积取得最大值</li>
</ul>
<p><img src="向量点积.jpg" alt=""></p>
<p>另外，可以认为内积表示两个向量在多大程度上指向相同方向。如果将方向相似判定为“相似”，则两个向量相似时内积变大。后面我们考察神经网络时，这个观点就变得十分重要。</p>
<p><img src="向量相似度.png" alt=""></p>
<p>向量的一般化 </p>
<p>我们平常见到的多是平面（二维空间）以及三维空间中的向量。向量的方便之处就在于，二维及三维空间中的性质可以照搬到任意维空间中。神经网络索然要处理数万维的空间，但是二维空间以及三维空间中的向量性质可以直接利用，处于该原因，向量被充分应用于梯度下降法中。</p>
<p>张量（tensor）时向量概念的推广，谷歌提供的人工智能学习系统TensorFlow的命名就用到了这个数学术语</p>
<p>tensor来源于tension（物理学中的张力）。想固体施加张力时，会在固体的截面产生力的作用，这个力称为应力。这个力在不同的界面上大小和方向各不相同。</p>
<p><img src="张量.png" alt=""></p>
<p>因此当法向为x、y、z轴时，作用在上面的力依次用向量表示为。</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
 T_{11}\\
 T_{21}\\
 T_{31}
\end{bmatrix},
\begin{bmatrix}
 T_{12}\\
 T_{22}\\
 T_{32}
\end{bmatrix},
\begin{bmatrix}
 T_{13}\\
 T_{23}\\
 T_{33}
\end{bmatrix}</script><p>可以将它们合并为以下矩阵</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
  T_{11}& T_{12} & T_{13}\\
  T_{21}& T_{22} & T_{23}\\
  T_{31}& T_{32} & T_{33}
\end{bmatrix}</script><p>我们称这个量为应力张量。</p>
<p>张量值应力张量在数学上的抽象。我们不清楚谷歌用TensorFlow明明人工智能学习系统的原委，不过在神经网络的世界里，经常用到附带多个下标的变量，这与张量的计算相似，可能也是出于这个原因，TensorFlow才这样命名吧。</p>
<p>需要区分张量中的维度与阶数的概念。具体地说，一个张量总由m^n个数值分量构成，其中m是维度数，n是阶数。</p>
<p>零阶张量就是平时说的标量，一阶张量就是向量，二阶张量就是矩阵。</p>
<p>多元函数</p>
<p>多元函数难以直观化，描述神经网络的函数的变量有成千上万个，因此难以从直观上理解这些函数。不过，只要了解了单变量的情况，我们就可以将多变量的情况作为其扩展来理解。</p>
<p>偏导数</p>
<p>求导的方法也同样适用于多变量函数的情况。但是，由于有多个白能量，所以必须知名对哪一个变量进行求导。在这个意义上，关于某个特定变量的导数就称为偏导数（partial derivative）。</p>
<p>多元函数的最小值条件</p>
<p>光滑的一元函数在某点取得最小值的必要条件是导函数在该点取值为0，事实上这对于多元函数同样适用。因此多元函数在某点取最小值的必要条件是函数在该点对各变量求偏导的值为0。在实际的最小值问题中，又是会对变量附加约束条件，这种情况下我们可以使用拉格朗日乘数法，在用于求性能良好的神经网络的正则化技术中，经常使用该方法。</p>
<p>梯度下降法思路</p>
<p>链式求导法则不再赘述，求二元函数的最小值</p>
<script type="math/tex; mode=display">
z=f(x,y)\to \frac{\partial f(x,y)}{\partial x} =0,\frac{\partial f(x,y)}{\partial y} =0</script><p><img src="偏导.png" alt=""></p>
<p>在实际问题中，偏导公式通常不容易求解，梯度下降法（最速下降法）是一种具有代表性的替代方法。</p>
<script type="math/tex; mode=display">
\Delta z=f(x+\Delta x,\Delta y)=
\frac{\partial f(x,y)}{\partial x} \Delta x+
\frac{\partial f(x,y)}{\partial y} \Delta y</script><p>上式可以表达为向量的内积形式</p>
<script type="math/tex; mode=display">
\left ( \frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y}  \right ).
\left ( \Delta x,\Delta y \right )</script><p>注意这个内积的关系，这就是梯度下降法的出发点。我们知道，当两个向量的方向相反时，它们的内积取得最小值（负数），所以，当两个向量满足下式时，函数值减小得最快</p>
<script type="math/tex; mode=display">
\left ( \Delta x,\Delta y  \right ) = -\eta \left ( \frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y}  \right )  \eta为正的微小常数</script><p>右边的向量称为函数在点（x，y）处的梯度（gradient）。</p>
<p>寻找最小值时我们线选定一点，求出函数在该点的梯度，最后求出偏移向量并进行偏移，重复此过程直至找到梯度为0的点，求出最小值。</p>
<script type="math/tex; mode=display">
\left ( x+\Delta x,y+\Delta y \right )</script><p>上述公式可以推广到任意维度的向量</p>
<script type="math/tex; mode=display">
\left ( \Delta x_{1},\Delta x_{2},……,\Delta x_{n}  \right ) =
-\eta\left ( \frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}},
……,\frac{\partial f}{\partial x_{n}} \right )</script><p>哈密顿算子</p>
<p>在实际的神经网络中，主要处理由成千上万个变量构成的函数的最小值。在这种情况下，梯度向量的表达会变得十分冗长，需要用更简洁的表示方法。在数学中，有一个被称为向量分析的领域，其中有一个经常用到的符号，称为哈密顿算子，其定义如下：</p>
<script type="math/tex; mode=display">
\nabla f=\left ( \frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}},
……,\frac{\partial f}{\partial x_{n}} \right )\bigtriangledown 为哈密顿算子</script><script type="math/tex; mode=display">
\left ( \Delta x_{1},\Delta x_{2},……,\Delta x_{n}  \right ) =
-\eta\bigtriangledown f</script><p>η的含义以及梯度下降法的要点</p>
<p>到目前为止，η只是简单地表示正的微小常数。而在实际使用计算机进行计算时，如何恰当的确定这个η是个大问题。从前面的式子易知，η是在函数上移动的步长，根据η的值，可以确定下一步移动到那个点，如果步长会较大，那么可能会达到最小值点，也可能会直接跨过了最小值点（左）。而如果步长较小，则可能会滞留在极小值点（右）</p>
<p><img src="梯度下降法η值的确定.png" alt=""></p>
<p>在神经网络中，<strong>η称为学习率</strong>。遗憾的是，它的确定方法没有明确的标准，只能通过反复更改来寻找恰当的值。</p>
<h1 id="最优化问题与回归分析"><a href="#最优化问题与回归分析" class="headerlink" title="最优化问题与回归分析"></a>最优化问题与回归分析</h1><p>在为了分析数据而建立数学模型时，模型是由参数确定的。在数学世界中，最优化问题就是如何确定这些参数。</p>
<p>从数学上来说，确定神经网络的参数是一个最优化问题，具体是对神经网络的参数（权重和偏置）进行<strong>拟合</strong>，使得神经网络的输出与实际数据相吻合。</p>
<p>Tips：</p>
<p>形象的说，拟合就是把平面上一系列的点，用一条光滑的曲线连接起来。因为这条曲线有无数种可能，从而有各种拟合方法。拟合的曲线一般可以用函数表示，根据这个函数的不同有不同的拟合名字。</p>
<p>为了理解最优化问题，最浅显的例子就是回归分析。</p>
<h2 id="回归分析"><a href="#回归分析" class="headerlink" title="回归分析"></a>回归分析</h2><p>在由多个变量组成的数据中，着眼于其中一个特定的变量，用其余的变量来解释这个特定的变量，这样的方法称为回归分析。回归分析的种类有很多，其中最简单的就是一元线性回归分析。</p>
<p><img src="一元线性回归.png" alt=""></p>
<script type="math/tex; mode=display">
y=px+q(p、q为常数)</script><p>x是自变量，y为因变量，常数p、q是这个回归分析模型的参数，由给出的数据来决定。（p称为回归系数，q为截距）</p>
<p>下面我们通过具体的例子来看看回归方程是如何确定的。</p>
<p>下面是一组身高与体重数据，根据这些数据求以体重y为因变量，身高x为自变量的回归方程y=px+q（p、q为常数）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>身高x</th>
<th>体重y</th>
<th>预测值p+q</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>153.3</td>
<td>45.5</td>
<td>153.3p+q</td>
</tr>
<tr>
<td>2</td>
<td>164.9</td>
<td>56.0</td>
<td>164.9p+q</td>
</tr>
<tr>
<td>3</td>
<td>168.1</td>
<td>55.0</td>
<td>168.1p+q</td>
</tr>
<tr>
<td>4</td>
<td>151.5</td>
<td>52.8</td>
<td>151.5p+q</td>
</tr>
<tr>
<td>5</td>
<td>157.8</td>
<td>55.6</td>
<td>157.8p+q</td>
</tr>
<tr>
<td>6</td>
<td>156.7</td>
<td>50.8</td>
<td>156.7p+q</td>
</tr>
<tr>
<td>7</td>
<td>161.1</td>
<td>56.4</td>
<td>161.1p+q</td>
</tr>
</tbody>
</table>
</div>
<p>y的实测值与预测值。在考虑数学上的最优化问题时，理解实测值与预测值的差异是十分重要的。</p>
<script type="math/tex; mode=display">
e_{k}=y_{k}-(px_{k}+q) //实际体重y_{k}预测值的误差e_{k},可正可负\\
C_{k}= \frac{1}{2} (e_{k})^2=\frac{1}{2}\left \{ y_{k}-(px_{k}+q)^2 \right \} //平方误差\\
C_{T}=C_{1}+C_{2}+……+C_{n}//总误差和\\</script><p>我们的目标是确定常数p、q的值。回归分析任务p、q是使总误差和最小的解。知道这个思路后解题就很简单了。即</p>
<script type="math/tex; mode=display">
\frac{\partial C_{T}}{\partial p}=0,\frac{\partial C_{T}}{\partial q}=0</script><p><img src="一元线性回归2.png" alt=""></p>
<p>以上就是一元线性回归分析中使用的回归直线的确定方法，这里的重点是最优化问题的求解思路。这里所考察的最优化方法在神经网络计算中也可以直接使用。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>在最优化方面，误差综合可以称为误差函数、随时函数、代价函数等。这里采用代价函数（cost function）叫法。因为误差函数（error function）和损失函数（loss function）的首字母容易与神经网络中用到的熵（entropy）、层（layer）的首字母混淆。</p>
<p>此外，除了这里所说的平方误差的综合之外，根据不同的思路，代价函数还存在其他多种形式。利用平方误差的总和进行最优化的方法称为最小二乘法，目前就让我们先只考虑将平方误差综合作为代价函数。</p>
<h1 id="神经网络的最优化"><a href="#神经网络的最优化" class="headerlink" title="神经网络的最优化"></a>神经网络的最优化</h1><p>前面我们已经初步了解了神经网络的思想和工作原理。不过，要在数学上实际地确定其权重和偏置，必须将神经网络的思想用具体的式子表示出来。</p>
<h2 id="参数和变量"><a href="#参数和变量" class="headerlink" title="参数和变量"></a>参数和变量</h2><p>从数学上看，神经网络是一种用于数据分析的模型，这个模型是由权重和偏置确定的。相权重和偏置这种确定数学模型的常熟称为模型的参数。</p>
<p>除了参数以外，数据分析的模型还需要根据数据而变化的变量，但是参数和变量都用拉丁字母或希腊字母标记，这回引起混乱。而区分用于代入数据值的变量和用于确定模型的参数，对于逻辑的理解是不可或缺的。</p>
<p>在实际进行神经网络的计算时，往往会倍数量庞大的参数和变量所困扰。构成神经网络的神经单元的数量非常大，相应地表示偏置、权重、输入、输出的变量的数目也变得非常庞大。因此，参数和变量的表示需要统一标准（之前的表示方法都没有考虑统一性）。</p>
<p>下面我们来确认神经网络中变量和参数的表示方法。</p>
<p>首先，我们对层进行编号，如下图，最左边的输入层为层1，隐藏层为层2、层3……最右边的输出层为层l（l指的是last，表示层的总数）。</p>
<p><img src="阶层型神经网络各层的名称.png" alt=""></p>
<script type="math/tex; mode=display">
\begin{aligned}
&x_{i}:表示输入层(层1)的第i个神经单元的输入变量。由于输入层的神经单元的输入和输出为同一值，所以也是表示输出的变量。
此外，这个\\
&\qquad变量名也作为神经单元的名称使用\\
&w_{ji}^{l}:从层l-1的第i个神经元指向层l的第j个神经元的箭头的权重。请注意i和j的顺序。这是神经网络的参数\\
&z_{j}^{l}:表示层l的第j个神经元的输入变量\\
&b_{j}^{l}:层l的第j个神经元的偏置。这是神经网络参数\\
&a_{j}^{l}:层i的第j个神经元的输出变量。此外，这个变量名也作为神经单元的名称使用。
\end{aligned}</script><p><img src="参数与变量的表示方法.png" alt=""></p>
<h2 id="学习数据和正解"><a href="#学习数据和正解" class="headerlink" title="学习数据和正解"></a>学习数据和正解</h2><p>利用事先准备好的数据（学习数据）来确定权重和偏置。这在神经网络中称为学习。学习的逻辑非常简单，使神经网络算出的预测值与学习数据的正解的总体误差达到最小即可。</p>
<h1 id="神经网络的和误差反向传播法"><a href="#神经网络的和误差反向传播法" class="headerlink" title="神经网络的和误差反向传播法"></a>神经网络的和误差反向传播法</h1><p>沿着最陡的坡度下山，就能以最少的部署到达山脚。梯度下降法就是将这个原理哟应用在数学上的数值分析方法。未来求出梯度的方向，需要进行求导，但在神经网络中，倒数计算的计算量非常大。误差反向传播法就解决了这个难题。</p>
<p>求函数最小值的通用方法中，最有名的就是利用最小值条件（导数为值0）。</p>
<script type="math/tex; mode=display">
z=f(x,y)\Longrightarrow \frac{\partial f}{\partial y} =0,\frac{\partial f}{\partial x}=0</script><p>在神经网络中，代价函数就相当于f，权重和偏置相当于变量x、y。权重和偏置的总数十分庞大，而且代价函数中包含了激活函数，所以求解这样的方程是十分困难的。</p>
<p>例题：</p>
<p>已知一个用于识别通过4×3像素的图像读取的手写数字0、1的神经网络，其代价函数为 G 。尝试进行求代价函数最小值的计算。学习用的图像数据为64张图像，像素为单色二值。<br>前面已经考察过，我们可以建立如下图所示的神经网 作为这个例题的解。<br>注：神经单元名使用的是输出变量名。</p>
<p><img src="神经网络例题.png" alt=""></p>
<p>第一层与第二层之间有12X3=36个权重和3个偏置，第二层与第三层之间有6个权重和2个偏置，<strong>此神经网络共有47个参数</strong>。</p>
<p>列出这个神经网络的关系是，其中激活函数为a(z)。</p>
<p><img src="神经网络例题2.jpg" alt=""></p>
<p>此外，神经网络计算出的预测值（a）与学习数据的正解（t）的平方误差C如下所示：</p>
<script type="math/tex; mode=display">
C=\frac{1}{2}\left \{ (t_{1}-a_{1}^{3} )^2 + (t_{2}-a_{2}^{3}) \right \}</script><p><strong>将n个实例输入到这个神经网络中，得到代价函数，现在的主角就是这个代价函数。</strong></p>
<script type="math/tex; mode=display">
C_{T}=C_{1}+C_{2}+……+C_{n}</script><p><img src="神经网络例题3.jpg" alt=""></p>
<p>已知该神经网络要确定的参数共有47个，想要通过求偏导零点来确定参数就需要47个方程。</p>
<p><img src="神经网络例题4.png" alt=""></p>
<p>求解这些方程是极其困难的，于是梯度下降法应运而生。</p>
<p>把函数图像看作斜坡，沿着坡度最陡的方向一步一步地下降，将这个想法在数学上表示出来，就是梯度下降法。</p>
<script type="math/tex; mode=display">
x_{1}+\Delta x_{1},x_{2}+\Delta x_{2},……,x_{n}+\Delta x_{n}</script><p>当以下关系式成立时，函数f减小得最快。η为正的微小常数</p>
<script type="math/tex; mode=display">
\left ( \Delta x_{1},\Delta x_{2},……,\Delta x_{n}  \right ) =
-\eta\left ( \frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}},
……,\frac{\partial f}{\partial x_{n}} \right )</script><p>我们试着将上式应用到上面展示的神经网络中</p>
<script type="math/tex; mode=display">
(\Delta w_{11}^{2},…,\Delta w_{11}^{3},…,\Delta b_{1}^{2},…,\Delta b_{1}^{3},…)=
-\eta (\frac{\partial C_{T}}{\partial w_{11}^{2}},…,\frac{\partial C_{T}}{\partial w_{11}^{3}} 
,...,\frac{\partial C_{T}}{\partial b_{1}^{2}},...,\frac{\partial C_{T}}{\partial b_{1}^{3}},...)</script><p>用计算机计算这个梯度的分量使十分麻烦的。仅计算一张图像的平方误差对一个参数的偏导就要进行如下计算：</p>
<script type="math/tex; mode=display">
\frac{\partial C_{k}}{\partial w_{11}^{2}} =\frac{\partial C_{k}}{\partial a_{1}^{3}[k]} 
\frac{\partial a_{1}^{3}[k]}{\partial z_{1}^{3}[k]} 
\frac{\partial z_{1}^{3}[k]}{\partial a_{1}^{2}[k]} 
\frac{\partial a_{1}^{2}[k]}{\partial z_{1}^{2}[k]} 
\frac{\partial z_{1}^{2}[k]}{\partial w_{11}^{2}[k]}+ \\
\frac{\partial C_{k}}{\partial a_{2}^{3}[k]} 
\frac{\partial a_{2}^{3}[k]}{\partial z_{2}^{3}[k]} 
\frac{\partial z_{2}^{3}[k]}{\partial a_{1}^{2}[k]} 
\frac{\partial a_{1}^{2}[k]}{\partial z_{2}^{2}[k]} 
\frac{\partial z_{1}^{2}[k]}{\partial w_{11}^{2}[k]}</script><p><img src="神经网络例题5.jpg" alt=""></p>
<p>若学习数据有64张图，则仅对一个参数求偏导就要将上面的过程还要再重复64次。</p>
<p><img src="神经网络例题6.png" alt=""></p>
<p>由此我们可以知道，用具体的式子来求梯度分量是非常困难的。未来解决这个问题，人们研究出了误差反向传播法。</p>
<p>通过上面的计算，我们了解到：梯度分量是一个个学习实例的简单和。也就是说，<strong>代价函数的偏导数是从各个学习实例中得到的偏导数的和。</strong></p>
<h2 id="神经元误差"><a href="#神经元误差" class="headerlink" title="神经元误差"></a>神经元误差</h2><p>梯度下降法对于寻找多变量函数的最小值的问题是有效的。然而在神经网络中，变量、参数和函数错综复杂，无法直接使用梯度下降法，于是出现了误差反向传播法。</p>
<p>误差反向传播法的特点是将繁杂的导数计算替换成数列的递推关系式，而提供这些递推关系式的就是名为神经单元误差（error）的变量。利用平方误差C，其定义如下所示：</p>
<script type="math/tex; mode=display">
\delta_{j}^{l}=\frac{\partial C}{\partial z_{j}^{l}}</script><p>虽然神经单元误差和平方误差同为误差，但它们的含义却不一样。</p>
<p><img src="误差反向传播法.jpg" alt=""></p>
<script type="math/tex; mode=display">
用\delta_{j}^{i}来表示\frac{\partial C}{\partial w_{11}^{3}} :\\
\frac{\partial C}{\partial w_{11}^{3}}=\frac{\partial C}{\partial z_{11}^{3}}\frac{\partial z_{11}^{3}}{\partial w_{11}^{3}}\\
z_{1}=w_{11}^{3}a_{1}^{2}+w_{12}^{3}a_{2}^{2}+w_{13}^{3}a_{3}^{2}\\
\frac{\partial z_{1}^{3}}{\partial w_{11}^{3}}=a_{1}^{2}\\
\frac{\partial C}{\partial w_{11}^{3}}=\delta_{1}^{3}a_{1}^{2}</script><p><img src="误差反向传播法2.png" alt=""></p>
<p>通过同样计算我们可以算出：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial b_{1}^{2}}=\frac{\partial C}{\partial z_{1}^{2}}=\delta_{1}^{2}</script><p>我们可以得到如下的一般式：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w_{ji}^{l}}=\delta_{j}^{l}a_{i}^{l-1},\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}</script><h2 id="误差反向传播算法"><a href="#误差反向传播算法" class="headerlink" title="误差反向传播算法"></a>误差反向传播算法</h2><p>如果我们能得到神经单元误差，就可以得到作为梯度下降法基础的平方误差的偏导数。那么如何求神经单元误差呢？这里我们利用数学中的数列递推关系式思想。</p>
<p>数列为数的序列，其第一项为首相，最后一项为末项。有趣的是，将神经单元误差看作数列是，可以简单地求出它的“末项”。</p>
<p>现在我们考虑的例子中，神经网络的层数为3，因此我们试着计算数列末项的误差，以a(z)为激活函数，根据链式法则有：</p>
<script type="math/tex; mode=display">
\delta_{j}^{3}=\frac{\partial C}{\partial z_{j}^{3}}=\frac{\partial C}{\partial a_{j}^{3}}
\frac{\partial a_{j}^{3}}{\partial z_{j}^{3}}=\frac{\partial C}{\partial a_{j}^{3}}{a}'(z_{j}^{3})</script><p>像这样，如果给出平方误差C和激活函数，就可以具体地求出相当于“末项”的输出层神经误差。以L作为输出层的编号，将上面的式子一般化，如下所示：</p>
<script type="math/tex; mode=display">
\delta_{j}^{L}=\frac{\partial C}{\partial a_{j}^{L}}{a}'(z_{j}^{L})</script><h3 id="中间层的“反向”递推关系式"><a href="#中间层的“反向”递推关系式" class="headerlink" title="中间层的“反向”递推关系式"></a>中间层的“反向”递推关系式</h3><p>神经元误差具有非常好的性质。它通过简单的关系式与下一层的神经元误差联系起来.</p>
<script type="math/tex; mode=display">
\delta_{1}^{2}=\frac{\partial C}{\partial z_{1}^{2}}=
\frac{\partial C}{\partial z_{1}^{3}}\frac{\partial z_{1}^{3}}{\partial a_{1}^{2}}\frac{\partial a_{1}^{2}}{\partial z_{1}^{2}}
+\frac{\partial C}{\partial z_{2}^{3}}\frac{\partial z_{2}^{3}}{\partial a_{1}^{2}}\frac{\partial a_{1}^{2}}{\partial z_{1}^{2}}\\
\frac{\partial C}{\partial z_{1}^{3}}=\delta_{1}^{3},\frac{\partial C}{\partial z_{2}^{3}}=\delta_{2}^{3} \\
\frac{\partial z_{1}^{3}}{\partial a_{1}^{2}}=w_{11}^{3},\frac{\partial z_{2}^{3}}{\partial a_{1}^{2}}=w_{21}^{3}\\
\frac{\partial a_{1}^{2}}{\partial z_{1}^{2}}={a}'(z_{1}^{2})\\
\delta_{1}^{2}=\delta_{1}^{3}w_{11}^{3}{a}'(z_{1}^{2})+\delta_{2}^{3}w_{21}^{2}{a}'(z_{1}^{2})\\
\delta_{1}^{2}=(\delta_{1}^{3}w_{11}^{3}+\delta_{2}^{3}w_{21}^{3}){a}'(z_{1}^{2})</script><p><img src="误差反向传播法3.png" style="zoom: 50%;"></p>
<p>由此我们可以得出当前层与下一层的一般关系式：</p>
<script type="math/tex; mode=display">
\delta_{i}^{l}=(\delta_{1}^{l+1}w_{li}^{l+1}+\delta_{2}^{l+1}w_{2i}^{l+1}+…
+\delta_{m}^{l+1}w_{mi}^{l+1}){a}'(z)\\
m为层l+1的神经单元个数，l为2以上的整数</script><p>这样我们就可以从输出层开始反向求出各层的神经元误差，进而求出代价函数对权重和偏置的偏导。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>建立一个6x6像素的图像读取数字1、2、3.图像的像素值为单色二值。</p>
<p>图中用圆圈将变量名圈起来的就是神经元，从图中我们可以了解到卷积神经网络的特点。隐藏层由多个具有结构的层组成。具体来说，<strong>隐藏层是由多个卷积层和池化层组成的</strong>。它不仅深，而且含有内置结构。卷积层的英文是convolution layer。这里展示的是最简单的卷积神经网络。</p>
<p><img src="卷积神经网络.png" style="zoom: 67%;"></p>
<p>我们设置一些过滤器，也就是图中的小恶魔，它们有自己对应的偏好模式，会对特定的模式的神经元起反应，而且它是活跃的，会积极地从图像中找出偏好模式。</p>
<p>为了让这些过滤器能够活动，我们为其提供工作场所，那就是由卷积层和池化层构成的隐藏子层。我们为每个过滤器准备一个隐藏子层作为工作场所。</p>
<p><img src="卷积神经网络(2).png" style="zoom:67%;"></p>
<p>活跃的过滤器积极地扫描图像，检查图像中是否含有自己偏好的模式。如果图像中含有较多偏好的模式，过滤器就起反应，反之就不起反应。此外，由于偏好的模式的大小比整个图像小，所以兴奋度被记录在多个神经单元中。</p>
<p><img src="卷积神经网络(3).png" style="zoom:50%;"></p>
<p><img src="卷积神经网络(4" alt="">.png)</p>
<p>注：一般用于扫描的过滤器大小是5x5。这里为了使结果变简单，我们使用3x3大小。</p>
<p>活跃的过滤器进一步整理自己的兴奋度，将兴奋度集中起来，整理后形成了池化层。</p>
<p><img src="卷积神经网络(5).png" style="zoom:50%;"></p>
<p>因此池化层的神经元中浓缩了作为考察对象的图像中包含了多少过滤器所偏好的模式这一信息。</p>
<p>要识别1、2、3，就需要让多个小恶魔登场。这里我们比较随意地假定有3个过滤器，输出层将这3个过滤器的报告组合起来，得出整个神经网络的判定结果。</p>
<p><img src="卷积神经网络(6).png" style="zoom:50%;"></p>
<p>与前面学的神经网络不同，这里的过滤器是动态的，它们会积极扫描图像，整理兴奋度并向下一层汇报。由于这些特点，卷积神经网络有我们前面学习的简单神经网络所没有的确定。</p>
<ul>
<li>对于复杂的模式识别问题，也可以用简洁的网络来处理。</li>
<li>整体而言，因为神经单元的数量少了，使用计算比较轻松。</li>
</ul>
<h2 id="过滤器的数量"><a href="#过滤器的数量" class="headerlink" title="过滤器的数量"></a>过滤器的数量</h2><p>过滤器的数量是不确定的，如果我们预估用5个模式能过区分图像，那么就需要有5个过滤器。这样一来，我们就应当准备好5个由卷积层和池化层形成的隐藏子层。</p>
<p><img src="卷积神经网络过滤器数量.png" style="zoom: 33%;"></p>
<p>而且，在识别复杂图像的情况下，隐藏层的结构本身也需要变得更复杂。这就需要设计人员大展身手了。</p>
<h2 id="过滤器的工作过程"><a href="#过滤器的工作过程" class="headerlink" title="过滤器的工作过程"></a>过滤器的工作过程</h2><p>我们来观察一个过滤器的工作，假设过滤器偏好模式如下：</p>
<p><img src="卷积_偏好模式.png" style="zoom: 25%;"></p>
<p>假设下面的图像“2”就是要考察的图像，我们将手写数字2作为它的正解。</p>
<p><img src="卷积_数字识别.png" style="zoom: 25%;"></p>
<p>首先过滤器对图像进行扫描。</p>
<p><img src="卷积_过滤器扫描.jpg" style="zoom: 33%;"></p>
<p>各个图像下面的“相似度”表示过滤器的灰色各自部分与扫描图像块的灰色各自部分吻合的地方的个数。这个值越大，就说明越符合过滤器的偏好模式。（这个是像素为二值0或1时的情况，更一般的情况后面再讨论）</p>
<p>我们将这个相似度汇总一下，如下所示，就是根据过滤器得到的卷积（convolution）的结果，称为特征映射（feature map）。</p>
<p><img src="卷积_特征映射.png" style="zoom:25%;"></p>
<p>这样的过滤器计算称为卷积。</p>
<p>卷积层中的神经单元将这一卷积的结果作为输入信息。个神经单元将对应的卷积值加上特征映射固有的偏置作为加权输入。</p>
<p><img src="卷积_加权输入.png" style="zoom: 50%;"></p>
<p>卷积层的各个神经单元通过激活函数来处理加权输入，并将处理结果作为神经单元的输出。这样卷积层的处理就完成了。</p>
<p><img src="卷积_输入处理.png" style="zoom:33%;"></p>
<h2 id="通过池化进行信息压缩"><a href="#通过池化进行信息压缩" class="headerlink" title="通过池化进行信息压缩"></a>通过池化进行信息压缩</h2><p>再实际图像情况下，卷积层神经单元的数目还是十分庞大的。因此需要进行信息压缩操作，然后将压缩结果放进池化层的神经单元中。</p>
<p>压缩的方法十分简单，只需要将卷积神经单元划分为相等大小不重叠的区域，然后在各区域中计算出代表值即可。这里我们使用最有名的信息压缩方法——最大池化（max polling），就是将划分好的各区域的最大值提取出来。</p>
<p><img src="卷积_最大池化.png" style="zoom: 33%;"></p>
<p>这样一来，一张图像的信息，就被集中在紧凑的神经单元集合中了。</p>
<p><img src="卷积_池化(2).png" style="zoom:33%;"></p>
<p><img src="卷积_池化(3).png" style="zoom:33%;"></p>
<p>如果一个过滤器的池化层神经单元的输出值较大，就表示原始图像中包含较多该过滤器的模式。</p>
<h2 id="卷积神经网络的变量关系式"><a href="#卷积神经网络的变量关系式" class="headerlink" title="卷积神经网络的变量关系式"></a>卷积神经网络的变量关系式</h2><p>前面我们初步了解了过滤器工作的数学思路，下面我们用数学式子表示出来。</p>
<h3 id="确定各层的含义以及变量明、参数名"><a href="#确定各层的含义以及变量明、参数名" class="headerlink" title="确定各层的含义以及变量明、参数名"></a>确定各层的含义以及变量明、参数名</h3><p>还是前面的那个神经网络，我们先总览一下这个网络：</p>
<p><img src="卷积神经网络总览.jpg" alt=""></p>
<p>我们把确定这个卷积神经网络所需的变量、参数的符号及其含义魂宗在下表中：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>位置</th>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入层</td>
<td>$x_{ij}$</td>
<td>神经单元中输入的图像像素（i行j列)的值。与输出值相同</td>
</tr>
<tr>
<td>过滤器</td>
<td>$w_{ij}^{Fk}$</td>
<td>用于建立第k个特征映射的过滤器的i行j列的值</td>
</tr>
<tr>
<td>卷积层</td>
<td>$z_{ij}^{Fk}$</td>
<td>卷积层第k个子层的i行j列的神经单元的加权输入</td>
</tr>
<tr>
<td></td>
<td>$b^{Fk}$</td>
<td>卷积层第k个子层的i行j列的神经单元的偏置。注意这些偏置在同一层各特征映射中是相同的</td>
</tr>
<tr>
<td></td>
<td>$a_{ij}^{Fk}$</td>
<td>卷积层第k个子层的i行j列的神经单元的输出（激活函数的值）</td>
</tr>
<tr>
<td>池化层</td>
<td>$z_{ij}^{Pk}$</td>
<td>池化层第k个子层的i行j列的神经单元输入。通常是前一层输出值的非线性函数值</td>
</tr>
<tr>
<td></td>
<td>$a_{ij}^{Pk}$</td>
<td>池化层第k个子层的i行j列的神经单元的输出。与输入值$z_{ij}^{Pk}$一致</td>
</tr>
<tr>
<td>输出层</td>
<td>$w_{k-ij}^{On}$</td>
<td>从池化层第k个子层的i行j列的神经单元指向输出层第n个神经单元的箭头的权重</td>
</tr>
<tr>
<td></td>
<td>$z_n^o$</td>
<td>输出层第n个神经单元的加权输入</td>
</tr>
<tr>
<td></td>
<td>$b_n^o$</td>
<td>输出层第n个神经单元的偏置</td>
</tr>
<tr>
<td></td>
<td>$a_n^o$</td>
<td>输出层第n个神经单元的输出（激活函数的值）</td>
</tr>
<tr>
<td>学习数据</td>
<td>$t_n$</td>
<td>正解为1时，$t_1$=1，$t_2$=0，$t_3$=0 <br>正解为2时，$t_1$=0，$t_2$=1，$t_3$=0 <br>正解为3时，$t_1$=0，$t_2$=0，$t_3$=1</td>
</tr>
</tbody>
</table>
</div>
<p><img src="卷积神经网络流程.jpg" alt=""></p>
<p>与神经网络不同的是，卷积神经网络中考虑的参数增加了过滤器这个新的成分。</p>
<h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>在输入层的神经单元中，输入值与输出值相同，有：</p>
<script type="math/tex; mode=display">
a_{ij}^I=x_{ij}</script><h3 id="过滤层和卷积层"><a href="#过滤层和卷积层" class="headerlink" title="过滤层和卷积层"></a>过滤层和卷积层</h3><p>由于过滤器的数值是通过对学习数据进行学习确定的，所以它们是模型的参数。<strong>过滤器（Filter）也称为核（Kernel）</strong></p>
<p><img src="卷积_过滤器计算.png" alt=""></p>
<p>由此可以求出卷积值：</p>
<script type="math/tex; mode=display">
c_{ij}^{Fk}=w_{11}^{Fk}x_{ij}+w_{12}^{Fk}x_{ij+1}+w_{13}^{Fk}x_{ij+2}+…+w_{33}^{Fk}x_{i+2\thinspace j+2}</script><p>这样得到的数值集合就形成特征映射。我们给这些卷积值加上一个不依赖预备$i、j$的输$b^{Fk}$</p>
<script type="math/tex; mode=display">
z_{ij}^{Fk}=c_{ij}^{Fk}+b^{Fk}</script><p><img src="卷积_加权输入计算.png" alt=""></p>
<p>考虑以$z_{ij}^{Fk}$作为加权输入的神经单元，这中神经单元的集合形成卷积层的一个子层。$b^{Fk}$为卷积层共同的偏置。</p>
<p>激活函数为$a(z)$，对于加权输入$z<em>{ij}^{Fk}$，神经单元的输入$a</em>{ij}^{Fk}$可以如下表示：</p>
<script type="math/tex; mode=display">
a_{ij}^{Fk}=a(z_{ij}^{Fk})</script><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>卷积神经网络中设置有用于压缩卷积层信息的池化层，在很多文献中，将特征有映射的2x2（不一定是2x2）个神经元压缩为1个神经元。通过一次池化操作，特征映射的神经元数就缩减到来原先的1/4。</p>
<p><img src="卷积_池化(4).png" style="zoom:33%;"></p>
<p>比较有名的是最大池化法。</p>
<p><img src="卷积_最大池化(2).png" style="zoom:33%;"></p>
<p>从神经网络观点来看，池化层也是神经单元的集合。不过，从计算方法可知，这些神经单元在数学上是非常简单的。通常的神经单元是从前一层的神经单元接收甲醛输入，儿池化层的神经单元不存在权重和偏置概念，也就是不具有模型参数。此外，由于输入和输出的值相同，所以也不存在激活函数的概念、从数学上说，激活函数$a(x)$可以认为是恒等式$a(x)=x$。这个特性和输出层的神经单元相似。</p>
<p><img src="卷积_池化层神经单元.png" style="zoom:33%;"></p>
<p>以上讨论的池化层性质可以用式子如下表示。k是子层编号，i、j为整数，取值必须使它们指定的参数有意义。</p>
<script type="math/tex; mode=display">
\left\{\begin{matrix}
z_{ij}^k=Max(a_{2i-1\thinspace 2j-1}^{Pk},a_{2i-1\thinspace 2j}^{Pk},
a_{2i\thinspace 2j-1}^{Pk},a_{2i\thinspace 2j}^{Pk},)\hfil\\
a_{ij}^{Pk}=z_{ij}^{Pk}\hfil
\end{matrix}\right.</script><p><img src="卷积_池化层计算.png" style="zoom:33%;"></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p> 输出层中的神经元接收来自池化层的说有神经单元的箭头（全连接）。这样就可以综合地考察池化层的神经单元的信息。</p>
<p> <img src="卷积_输出层.png" style="zoom: 33%;"></p>
<p>上图可以用下面的式子来表示：</p>
<script type="math/tex; mode=display">
z_n^O=w_{1-11}^{On}a_{11}^{P1}+w_{1-12}^{On}a_{12}^{P1}+…+w_{2-11}^{On}a_{11}^{P2}+
w_{2-12}^{On}a_{12}^{P2}+…+w_{3-11}^{On}a_{11}^{P3}+w_{3-12}^{On}a_{12}^{P3}+…+b_n^o</script><p>这里的系数$w<em>{k-ij}^{On}$为输出层第n个神经单元给池化层十九点的输出$a</em>{ij}^{Pk}(k=1,2,3;i=1,2;j=1,2)$分配的权重，$b_n^o$为输出层第$n$个神经单元的偏置。</p>
<p>我们具体写出$z_1^O$的式子：</p>
<script type="math/tex; mode=display">
z_1^O=w_{1-11}^{O1}a_{11}^{P1}+w_{1-12}^{O1}a_{12}^{P1}+…+w_{2-11}^{O1}a_{11}^{P2}+
w_{2-12}^{O1}a_{12}^{P2}+…+w_{3-11}^{O1}a_{11}^{P3}+w_{3-12}^{O1}a_{12}^{P3}+…+b_1^O</script><p><img src="卷积_输出层(2).png" style="zoom:33%;"></p>
<p>接下来考虑输出层神经单元的输出，它们形成了整个神经网络的输出。输出层的第n个神经单元的输出值为$a_n^O$，激活函数为$a(z)$，则：</p>
<script type="math/tex; mode=display">
a_n^O=a(z_n^O)</script><h3 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h3><p>现在我们考虑的神经网络中，输出层神经单元的3个输出为$a_1^O、a_2^O、a_3^O$，对应的学习数据正解分别为$t_1、t_2、t_3$。于是，平方误差C可以如下表示。</p>
<script type="math/tex; mode=display">
C=\frac{1}{2}(t_1-a_1^O)^2+(t_2-a_2^O)^2+(t_3-a_3^O)^2</script><p>注：系数$\frac{1}{2}$是为了简洁倒数计算，这个$\frac{1}{2}$会在求导的过程中被约掉，不同的文献可能会使用不同的系数，这个系数对结论没有影响。</p>
<p><img src="卷积_代价函数.png" style="zoom:33%;"></p>
<p>将输入第k个图像时的平方误差的值记为$C_k$，如下所示：</p>
<script type="math/tex; mode=display">
C_k=\frac{1}{2}(t_1[k]-a_1^O[k])^2+(t_2[k]-a_2^O[k])^2+(t_3[k]-a_3^O[k])^2</script><p>全体学习数据的平方误差的总和就是代价函数$C_T$。</p>
<script type="math/tex; mode=display">
C_T=C_1+C_2+…+C_n</script><p>由此我们得到了作为计算主角的代价函数$C_T$。接下来的目标就是求得使代价函数达到最小值的权重和偏置。</p>
<h3 id="其他池化方法"><a href="#其他池化方法" class="headerlink" title="其他池化方法"></a>其他池化方法</h3><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>最大池化</td>
<td>使用对象区域的最大值作为代表值的压缩方法</td>
</tr>
<tr>
<td>平均池化</td>
<td>使用对象区域的平均值作为代表值的压缩方法</td>
</tr>
<tr>
<td>L2池化</td>
<td>例如，对于4个神经单元的输出值$a_1、a_2、a_3、a_4$，使用$\sqrt{a_1^2+a_2^2+a_3^2+a_4^2}$作为代表值的压缩方法</td>
</tr>
</tbody>
</table>
</div>
<h1 id="卷积神经网络和误差反向传播法"><a href="#卷积神经网络和误差反向传播法" class="headerlink" title="卷积神经网络和误差反向传播法"></a>卷积神经网络和误差反向传播法</h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Blogs/about" rel="external nofollow noreferrer">Koito Yuu</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://koito-yuu153.github.io/Blogs/Blogs/2023/03/07/shen-du-xue-xi/">https://koito-yuu153.github.io/Blogs/Blogs/2023/03/07/shen-du-xue-xi/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Blogs/about" target="_blank">Koito Yuu</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Blogs/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Blogs/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/Blogs/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/Blogs/medias/reward/wechat.jpg" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Blogs/2024/03/21/hexo-de-shi-yong-jiao-cheng/">
                    <div class="card-image">
                        
                        
                        <img src="/Blogs/medias/featureimages/3.jpg" class="responsive-img" alt="hexo的使用教程">
                        
                        <span class="card-title">hexo的使用教程</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Koito Yuu
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                本篇&nbsp;<i class="far fa-dot-circle"></i>
            </div>
            <div class="card">
                <a href="/Blogs/2023/03/07/shen-du-xue-xi/">
                    <div class="card-image">
                        
                        
                        <img src="/Blogs/medias/featureimages/21.jpg" class="responsive-img" alt="深度学习笔记">
                        
                        <span class="card-title">深度学习笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Blogs/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Blogs/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Blogs/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Blogs/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Blogs/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Blogs/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Blogs/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="9479612817"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Blogs/libs/aplayer/APlayer.min.js"></script>
<script src="/Blogs/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Blogs/about" target="_blank">Koito Yuu</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Koito-Yuu153" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1181062873@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2810880357" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2810880357" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Blogs/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Blogs/libs/materialize/materialize.min.js"></script>
    <script src="/Blogs/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Blogs/libs/aos/aos.js"></script>
    <script src="/Blogs/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Blogs/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Blogs/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Blogs/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Blogs/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Blogs/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Blogs/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
